# ğŸš€ text2sql_backend

A **Hybrid AI Backend** that combines:

- ğŸ§  **Text-to-SQL** (Natural Language â†’ SQL)
- ğŸ“„ **Document RAG** (Retrieval-Augmented Generation)
- ğŸ”€ **Intelligent Query Routing** (SQL / Documents / Hybrid)
- ğŸ¤– **Local LLM using Ollama** (Gemma / LLaMA models)
- ğŸ—„ **Supabase** (PostgreSQL)
- ğŸ“š **ChromaDB** (Vector Store)

---

## ğŸ§  What This Project Does

The system intelligently understands user questions and decides:

- If it needs database data â†’ generates & runs SQL
- If it needs document knowledge â†’ uses RAG
- If it needs both â†’ combines them into one clean answer

**Example Hybrid Query:**

> How many customers do we have and explain refund policy?

**System Flow:**

1. Splits intent
2. Runs safe SQL query
3. Retrieves relevant document chunks
4. Uses Ollama LLM to merge both answers

---

## ğŸ— Architecture

```
User Query
   â†“
Intent Splitter
   â†“
SQL Service        RAG Service
   â†“                    â†“
Hybrid Combiner (LLM)
   â†“
Final Answer
```

---

## ğŸ›  Tech Stack

- FastAPI
- Supabase (PostgreSQL)
- ChromaDB
- Ollama (Local LLM)
- Python

---

## ğŸ“¦ Core Features

### âœ… Text-to-SQL

- Natural language â†’ SQL
- Schema-aware generation
- SQL validation
- `LIMIT` enforcement
- Single statement restriction

### âœ… Document RAG

- Upload PDF / TXT
- Automatic chunking
- Vector embeddings
- Semantic search
- Source attribution

### âœ… Hybrid Queries

- Proper LLM-based intent splitting
- Runs SQL + RAG independently
- Merges results using LLM
- Returns clean unified response

---

## ğŸ” Safety Features

- Multiple SQL statement blocking
- `SELECT`-only enforcement
- `LIMIT` auto-append
- Input validation
- Error handling

---

## ğŸš€ How To Run Locally

1. **Install Dependencies**

```sh
pip install -r requirements.txt
```

2. **Start Ollama**

Make sure Ollama is installed. Pull a model (example):

```sh
ollama pull gemma2:9b
```

Start Ollama (usually runs automatically on port `11434`).

3. **Setup Environment Variables**

Create a `.env` file:

```env
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key
OLLAMA_BASE_URL=http://localhost:11434
```

4. **Run FastAPI Server**

```sh
uvicorn app.main:app --reload
```

Open the interactive docs:

```
http://localhost:8000/docs
```

---

## ğŸ“„ API Endpoints

- `POST /upload` â€“ upload document for RAG.
- `POST /query` â€“ unified intelligent endpoint.

  **Example request:**

  ```json
  {
    "question": "How many customers do we have and explain refund policy?",
    "top_k": 3
  }
  ```

  **Returns:**
  - Route type
  - SQL result (if applicable)
  - RAG result (if applicable)
  - Final combined answer (for Hybrid)

- `POST /query/documents` â€“ RAG-only endpoint.
- `POST /query/sql` â€“ SQL-only endpoint.

---

## ğŸ§ª Example Queries

**SQL Only**

- How many customers do we have?
- What is total revenue?
- Who is the highest spending customer?

**RAG Only**

- What is the refund policy?
- Explain course duration.
- What is customer support process?

**Hybrid**

- How many customers do we have and explain refund policy?
- What is total revenue and describe pricing policy?
- Who is top customer and what are return rules?

---

## ğŸ“‚ Project Structure

```
app/
 â”œâ”€â”€ main.py
 â”œâ”€â”€ config.py
 â”œâ”€â”€ services/
 â”‚   â”œâ”€â”€ text_to_sql_service.py
 â”‚   â”œâ”€â”€ rag_service.py
 â”‚   â”œâ”€â”€ hybrid_combiner_service.py
 â”‚   â”œâ”€â”€ intent_splitter_service.py
 â”‚   â”œâ”€â”€ embedding_service.py
 â”‚   â”œâ”€â”€ vector_service.py
 â”‚   â”œâ”€â”€ document_service.py
 â”‚   â””â”€â”€ sql_schema_service.py
data/
 â”œâ”€â”€ chroma/
 â””â”€â”€ uploads/
```

---

## ğŸ¯ Why This Project Is Valuable

This project demonstrates:

- LLM System Design
- Hybrid AI Pipelines
- Safe SQL generation
- RAG implementation
- Backend architecture design
- Real-world AI integration

---

## ğŸ“ˆ Future Improvements

- Async parallel SQL + RAG execution
- Streaming responses
- Authentication layer
- Docker deployment
- Cloud hosting

---

## ğŸ† Learning Outcomes

Through this project I learned:

- How to integrate LLMs into backend systems
- How to build a hybrid AI architecture
- How to validate and secure AI-generated SQL
- How to combine structured + unstructured data reasoning

---

## ğŸ’¡ Built With

FastAPI Â· Supabase Â· ChromaDB Â· Ollama
